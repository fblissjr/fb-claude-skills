last updated: 2026-02-14

# session log: 2026-02-14

## plugin structure migration to marketplace model (v0.4.0)

### problem
README.md and all plugin READMEs had incorrect CLI commands for installation:
- Used `claude plugin add` (doesn't exist) instead of `claude plugin install`
- Used `--plugin` flag (doesn't exist) for selecting subfolder plugins from GitHub URLs
- Used `claude plugin remove` instead of canonical `claude plugin uninstall`
- Plugin manifests at `plugin.json` (root) instead of canonical `.claude-plugin/plugin.json`
- No marketplace.json, making GitHub-based installation impossible

### what was done

1. **Migrated plugin manifests**: moved `plugin.json` to `.claude-plugin/plugin.json` for all four installable plugins (cogapp-markdown, mcp-apps, plugin-toolkit, web-tdd)
   - Removed non-standard `skills` and `agents` array fields (auto-discovery handles these)
   - Added `repository` field to all manifests

2. **Created marketplace**: added `.claude-plugin/marketplace.json` at repo root
   - Lists all four installable plugins with correct `source` paths
   - Marketplace name: `fb-claude-skills`
   - Enables: `/plugin marketplace add fblissjr/fb-claude-skills`

3. **Rewrote all installation docs**:
   - Root README.md: marketplace-based GitHub install (recommended), local clone install, development/testing with `--plugin-dir`, correct uninstall commands
   - All four plugin READMEs: updated to marketplace install flow
   - CLAUDE.md: updated repo structure tree and installation section
   - Usage section: corrected namespaced skill invocations

4. **Version bump**: 0.3.1 -> 0.4.0 (pyproject.toml + CHANGELOG.md)

### references used
- `docs/claude-docs/claude_docs_plugins.md` (cached official docs)
- `docs/claude-docs/claude_docs_plugin_reference.md` (cached CLI reference)
- Live docs: code.claude.com/docs/en/plugins, discover-plugins, plugin-marketplaces
- `docs/analysis/skills_guide_structured.md` (skills guide extraction)

### additional cleanup (pass 2)

5. **Evaluated heylook-monitor**: confirmed it's an MCP App (standalone MCP server + HTML UI), not a Claude Code plugin. Correctly excluded from marketplace.

6. **Updated CLAUDE.md "Adding a new skill module" checklist**: changed required structure to `.claude-plugin/plugin.json`, added note about auto-discovery, added marketplace.json registration step.

7. **Updated skill-maintainer config.yaml**: added `discover-plugins` and `plugin-marketplaces` to watched pages (now relevant since repo is a marketplace).

8. **Ran skills-ref validate on all 6 skills**: all pass validation.

9. **Replaced docs/claude-docs/ with clean markdown**:
   - `claude_docs_plugins.md`: replaced raw HTML scrape with clean markdown from live docs
   - `claude_docs_skills.md`: replaced raw HTML scrape with clean markdown from live docs
   - `claude_docs_plugin_reference.md`: updated with clean markdown from live docs
   - `claude_docs_discover_plugins.md` (new): install/marketplace management reference
   - `claude_docs_plugin_marketplaces.md` (new): marketplace creation/distribution reference
   - All files include `<!-- source: -->` and `<!-- fetched: -->` headers for provenance

10. **Updated docs/analysis/skills_guide_analysis.md**: added plugin structure compliance section (all PASS for v0.4.0), marked web-tdd README.md issue as RESOLVED, updated date.

### files changed
- `.claude-plugin/marketplace.json` (new)
- `cogapp-markdown/.claude-plugin/plugin.json` (moved from `cogapp-markdown/plugin.json`)
- `mcp-apps/.claude-plugin/plugin.json` (moved from `mcp-apps/plugin.json`)
- `plugin-toolkit/.claude-plugin/plugin.json` (moved from `plugin-toolkit/plugin.json`)
- `web-tdd/.claude-plugin/plugin.json` (moved from `web-tdd/plugin.json`)
- `README.md`
- `CLAUDE.md`
- `CHANGELOG.md`
- `pyproject.toml`
- `cogapp-markdown/README.md`
- `mcp-apps/README.md`
- `plugin-toolkit/README.md`
- `web-tdd/README.md`
- `skill-maintainer/config.yaml`
- `docs/claude-docs/claude_docs_plugins.md` (replaced)
- `docs/claude-docs/claude_docs_skills.md` (replaced)
- `docs/claude-docs/claude_docs_plugin_reference.md` (replaced)
- `docs/claude-docs/claude_docs_discover_plugins.md` (new)
- `docs/claude-docs/claude_docs_plugin_marketplaces.md` (new)
- `docs/analysis/skills_guide_analysis.md`

### doc audit (pass 3)

11. **Created skill-maintainer/README.md**: was the only module without one. Covers usage, scripts, monitored sources, tracked skills, state format.

12. **Updated docs/README.md**: added claude-docs contents table (was just "skills page", now enumerates all 5 docs). Updated date.

13. **Updated CHANGELOG.md**: v0.4.0 entry now includes pass-2 and pass-3 work.

#### additional files changed
- `skill-maintainer/README.md` (new)
- `docs/README.md`
- `CHANGELOG.md`
- `internal/log/log_2026-02-14.md`

## duckdb-backed relational store (v0.5.0)

### problem
skill-maintainer stored all state in `state.json`, which overwrites in place (no history), can't be queried temporally, and doesn't compose across use cases. No token budget tracking, no session journaling, no validation audit trail.

### what was done

#### phase A: foundation
1. Added `duckdb>=1.0` to pyproject.toml, `uv sync` installed DuckDB 1.4.4
2. Created `skill-maintainer/scripts/store.py` (~1074 lines): DuckDB Store class with star schema (4 dimension tables, 7 fact tables, 5 views, 6 sequences), config.yaml dimension sync, record_*/get_* methods, backward-compatible export_state_json()
3. Created `skill-maintainer/scripts/migrate_state.py`: one-time state.json import with round-trip verification. Tested: 1 watermark, 6 pages, 1 file hash imported. Export matches original.

#### phase B: CDC migration
4. Migrated `docs_monitor.py`: replaced load_state/save_state with Store methods
5. Migrated `source_monitor.py`: same pattern with commit tracking
6. Migrated `check_freshness.py`: uses DuckDB v_skill_freshness view instead of JSON traversal
7. Migrated `apply_updates.py`: records update attempts and validations in DuckDB
8. Migrated `validate_skill.py`: records validation results in fact_validation
9. Migrated `update_report.py`: reads from DuckDB instead of state dict

#### phase C: new capabilities
10. Created `measure_content.py`: token budget tracker. Results: plugin-toolkit 4,713 tokens (OVER), skill-maintainer 33,596 (CRITICAL -- includes scripts), create-mcp-app 2,978 (OK), migrate-oai-app 2,173 (OK)
11. Created `journal.py`: session activity logger with append/ingest/query modes. JSONL buffer for hook performance (<50ms).

#### phase D: documentation
12. Updated `skill-maintainer/SKILL.md`: added budget, history, journal commands. Version 0.2.0.
13. Created `docs/internals/duckdb_schema.md`: full schema documentation
14. Updated `CHANGELOG.md`: v0.5.0 entry
15. Version bumped pyproject.toml to 0.5.0
16. Added `*.duckdb`, `*.duckdb.wal`, `journal.jsonl` to .gitignore

#### bugs fixed during implementation
- **TypeError: can't subtract offset-naive and offset-aware datetimes** in check_freshness.py: DuckDB returns naive timestamps, but datetime.now(timezone.utc) is aware. Fixed by adding tzinfo to naive timestamps.
- **CatalogException: lastval() does not exist** in store.py: DuckDB lacks PostgreSQL's lastval(). Fixed by returning 0 from record_* methods (no callers use returned IDs).

### files created
- `skill-maintainer/scripts/store.py`
- `skill-maintainer/scripts/migrate_state.py`
- `skill-maintainer/scripts/measure_content.py`
- `skill-maintainer/scripts/journal.py`
- `docs/internals/duckdb_schema.md`

### files modified
- `pyproject.toml` (added duckdb dep, version 0.5.0)
- `.gitignore` (duckdb files, journal.jsonl)
- `skill-maintainer/SKILL.md` (new commands, version 0.2.0)
- `skill-maintainer/scripts/docs_monitor.py`
- `skill-maintainer/scripts/source_monitor.py`
- `skill-maintainer/scripts/check_freshness.py`
- `skill-maintainer/scripts/apply_updates.py`
- `skill-maintainer/scripts/validate_skill.py`
- `skill-maintainer/scripts/update_report.py`
- `CHANGELOG.md`

### background research (3 agents)

Three research agents were launched in parallel to explore broader use cases for the star schema approach:

1. **code-architect agent**: analyzed 10 strategic use cases with complete schemas, SQL queries, and integration points. Top recommendations: agent memory/continuity (high value, medium complexity), token budget optimization (medium-high value, low complexity), quality tracking over time (high value, medium complexity). Proposed general-purpose "Agent State Store" library with core + extension modules.

2. **general-purpose agent**: wrote comprehensive research doc at `docs/analysis/data_centric_agent_state_research.md` (~650 lines). Analyzed 12+ projects (MemGPT, LangSmith, Braintrust, OpenTelemetry, etc.). Key finding: no existing project uses DuckDB as persistent state for LLM agent operations -- open gap. Ranked 6 application areas; session cost tracking scored highest (pain 9/10, relational fit 10/10).

3. **plan agent**: designed full Agent State Store architecture with core schema (5 dimensions + 7 fact tables + 7 views), extension mechanism via numbered SQL migrations, Claude Code hook integration (capture.sh -> JSONL -> flush -> DuckDB), skill for querying (`/agent-state`), and MCP server for programmatic access. Proposed packaging as Python library + Claude Code plugin + MCP server.

## data-centric agent state research

### what was done

Comprehensive research document on data-centric approaches to LLM agent state management, analyzing how the existing DuckDB-backed star schema in store.py could generalize into a reusable library and MCP server.

#### scope of research
- Analyzed 12+ projects/platforms: MemGPT/Letta, LangSmith, Braintrust, OpenTelemetry GenAI, Mem0, Zep, Phoenix/Arize, DuckDB ecosystem (MotherDuck, Evidence, Malloy, dbt-duckdb)
- Mapped 6 data patterns from traditional software to agent systems: CDC, event sourcing, SCD, star schema, CQRS, bus architecture
- Ranked 6 application areas by pain/fit/integration scores
- Designed MCP server tool surface (10 tools across operational, analytical, and context management categories)
- Proposed v1 library architecture with core + extension module pattern

#### key findings
1. No existing project uses DuckDB as a persistent state store for LLM agent operations -- this is an open gap
2. Conversation memory (MemGPT) and operational data (our approach) are complementary, not competing
3. The star schema's compounding value over time is the real moat -- no vector DB or flat file provides this trajectory
4. Session cost tracking is the highest-impact application (pain 9/10, relational fit 10/10)
5. Claude Code hooks provide the capture layer; MCP server provides the query layer
6. Current store.py is a solid foundation but needs fact_tool_call, dim_session, and fact_token_usage tables for general use

### files created
- `docs/analysis/data_centric_agent_state_research.md`
- `internal/log/log_2026-02-14.md` (this update)

## kimball dimensional model refactor (v0.6.0)

### problem
store.py (v0.5.0) used OLTP patterns: integer PKs with MAX(id)+1, hard FK constraints, 6 sequences for fact table IDs, no SCD Type 2 on dimensions, no metadata columns for lineage tracking. This doesn't compose well across repos (ccutils, star-schema-llm-context) and can't track dimension changes over time.

### what was done

#### phase A: store.py rewrite
1. Replaced all integer PKs on dimensions with MD5 hash surrogate keys (`_hash_key()`)
2. Added SCD Type 2 metadata on all dimension tables: effective_from, effective_to, is_current, hash_diff, record_source, created_at, session_id, last_verified_at
3. Added `_hash_diff()` for change detection on non-key attributes
4. Dropped all 6 sequences (seq_watermark, seq_change, seq_validation, seq_update, seq_measurement, seq_event)
5. Dropped all FK REFERENCES constraints
6. Removed PRIMARY KEY from fact tables (grain = composite dimension keys + timestamp)
7. Added metadata columns to all fact tables: inserted_at, record_source, session_id
8. Added meta_schema_version table for schema evolution tracking
9. Added meta_load_log table for operational visibility
10. Merged fact_session into fact_session_event (session boundaries are events, not separate table)
11. Updated all views to filter is_current = TRUE and join on hash_key
12. Added automatic v1 -> v2 migration (detects old schema, drops all v1 objects, creates v2)

#### phase B: consumer script updates
13. migrate_state.py: added --force flag, integrated with meta_load_log
14. source_monitor.py: explicit record_source='source_monitor'
15. journal.py: rewritten for merged session/event model
16. Other scripts (docs_monitor, check_freshness, apply_updates, validate_skill, update_report, measure_content): compatible without changes due to backward-compatible method signatures

#### phase C: bug fix -- SCD Type 2 PK violation
17. Discovered latent bug: PRIMARY KEY on dimension tables prevents SCD Type 2 from working. When _sync_dimensions() detects a hash_diff change, it closes the old row (UPDATE is_current=FALSE) and inserts a new row with the same hash_key. PRIMARY KEY (unique constraint) rejects the insert.
18. Fixed by removing PRIMARY KEY from all three dimension tables (dim_source, dim_skill, dim_page). hash_key remains TEXT NOT NULL. DuckDB's columnar storage doesn't benefit from PK indexes for join performance -- all queries use is_current=TRUE filter.

#### phase D: documentation
19. Rewrote docs/internals/duckdb_schema.md for v2 schema
20. Created docs/analysis/abstraction_analogies.md (summary + pointer to canonical version)
21. Updated CLAUDE.md: added selection-under-constraint design principle, dimensional model section, three-repo architecture
22. Updated README.md: added design philosophy section
23. Created ~/workspace/star-schema-llm-context/docs/library_design.md (shared library design)
24. Created ~/workspace/star-schema-llm-context/docs/abstraction_analogies.md (canonical home for unified framework)
25. Updated library_design.md with PK removal rationale

#### phase E: version and changelog
26. Bumped pyproject.toml to 0.6.0
27. Added v0.6.0 entry to CHANGELOG.md

### verification
- `migrate_state.py --force`: imports state.json into v2 schema, round-trip verification passes
- `store.py --stats`: shows v2 schema with all dimensions synced
- `store.py --history 30`: shows 7 historical changes
- `store.py --freshness`: all 4 skills showing correct data
- `store.py --export`: backward-compatible state.json output
- `check_freshness.py`: all 4 skills OK
- `measure_content.py --skill plugin-toolkit`: measures correctly

### key design decisions
- **No PK on dimensions**: SCD Type 2 requires multiple rows per entity. hash_key identifies the entity, is_current distinguishes current from historical.
- **Session boundaries as events**: fact_session_event.event_type='session_start'/'session_end' eliminates awkward FK from fact to fact.
- **No sequences anywhere**: deterministic hash keys for dimensions, no PKs for facts.
- **Degenerate session dimension**: session_id carried directly in fact rows, no dim_session table (high cardinality, natural key IS the interesting attribute).

### files created
- `docs/analysis/abstraction_analogies.md`
- `~/workspace/star-schema-llm-context/docs/library_design.md`
- `~/workspace/star-schema-llm-context/docs/abstraction_analogies.md`

### files modified
- `skill-maintainer/scripts/store.py` (rewrite + budget trend view)
- `skill-maintainer/scripts/migrate_state.py`
- `skill-maintainer/scripts/source_monitor.py`
- `skill-maintainer/scripts/journal.py` (rewrite)
- `docs/internals/duckdb_schema.md` (rewrite)
- `~/workspace/star-schema-llm-context/docs/library_design.md` (PK fix + Gemini expansion roadmap)
- `CLAUDE.md`
- `README.md`
- `CHANGELOG.md`
- `pyproject.toml`
- `internal/log/log_2026-02-14.md`

### post-implementation reflections

#### SCD Type 2 PK bug: lesson learned
The original plan specified `TEXT PRIMARY KEY` on dimension tables. This is wrong for SCD Type 2 because multiple rows share the same hash_key (one current, N historical). The bug was latent -- it would only trigger when config.yaml actually changes a dimension's non-key attributes, which is rare. But it would have been a runtime crash. The fix (removing PK) was verified with a test that creates two rows with the same hash_key and confirms the is_current filter works.

**Lesson**: SCD Type 2 and unique constraints on the entity identifier are fundamentally incompatible. Traditional Kimball warehouses use auto-incrementing integer surrogates that ARE unique per version. Our deterministic hash approach (hash of natural key) produces the same key for all versions. If we needed fast lookups, a composite index on (hash_key, is_current) would be the right approach, but DuckDB's columnar storage handles this efficiently without explicit indexes for our data volumes.

#### meta-cognition view (from Gemini feedback)
Added `v_skill_budget_trend` view and `--budget-trend` CLI flag. This enables the "am I getting fatter over time?" query that Gemini identified as a meta-cognition capability. With daily measurements, an agent can detect growth trends and proactively prune references before hitting budget limits.

#### Gemini expansion proposals integrated
Added three architectural expansion proposals from Gemini Deep Think feedback to `~/workspace/star-schema-llm-context/docs/library_design.md`:
1. **Runtime layer** (fact_tool_call + fact_token_usage): self-optimization via cost-aware routing
2. **Semantic layer** (dim_skill_embedding): fuzzy routing via DuckDB vss extension
3. **Graph layer** (dim_skill_dependency): impact analysis via recursive CTEs

#### what I'd do differently
1. **Start with no PKs on dimensions.** The plan assumed PK would work for SCD Type 2 because traditional Kimball uses auto-increment surrogates. With deterministic hash surrogates, the constraint is wrong. Should have caught this during planning.
2. **Test the SCD Type 2 code path early.** The _sync_dimensions() change-detection path was never triggered because config.yaml didn't change between runs. A unit test would have caught the PK bug immediately.
3. **The Gemini expansion proposals are the right next steps.** fact_tool_call for runtime observability is highest-value. Embeddings require more infrastructure. Graph layer builds naturally on existing skill_source_dep.

#### what's working well
- Backward-compatible method signatures meant 6 of 10 consumer scripts needed zero changes
- SCD Type 2 pattern is verified and working correctly
- Merged session model eliminated an entire table and its awkward FK
- Hash-based surrogate keys work well for multi-repo system (ccutils can compute same key without DB)

#### future sessions
- **star-schema-llm-context library extraction**: extract core primitives into `src/star_schema/` package
- **fact_tool_call**: runtime observability layer via Claude Code hooks -> JSONL -> DuckDB

## vision, skill, and use cases (v0.7.0)

### context
v0.6.0 complete (Kimball star schema in store.py, all scripts migrated). Design docs in star-schema-llm-context. Two independent critique agents (Claude + Gemini) analyzed the approach. This session synthesized those critiques and executed the next steps.

### critique synthesis (key findings)
- Both critiques agree: "abstract the data, not the behavior" avoids the framework graveyard
- Both say: don't build the library yet, build the skill first
- Identity crisis in star-schema-llm-context: three projects in one repo (dead KG prototype, unimplemented library design, theoretical framework)
- My disagreement: "selection under constraint" is a transferable vocabulary, not just resource allocation
- What critiques missed: Anthropic will capture easy wins (native persistence, cost tracking). Build things that complement the harness, not compete with it.

### phase 1: star-schema-llm-context cleanup
1. Deleted ~3950 lines of dead code: graph_algorithms.py (854 lines), mcp_server.py (1123 lines), schema.sql (711 lines), db_manager.py (119 lines), setup.py (200 lines), Makefile (138 lines), ARCHITECTURE.md, requirements.txt, config.yaml (202 lines)
2. Rewrote README.md with clear vision statement (pattern library, not code library)
3. Rewrote CLAUDE.md to reflect current state (design docs, not running code)
4. Created pyproject.toml (uv-based, no dependencies yet)
5. Replaced speculative expansion roadmap in library_design.md: cut embeddings/vector search and full graph DB, replaced with DAG execution model and automation patterns

### phase 2: dimensional-modeling plugin (fb-claude-skills)
1. Created plugin directory structure: dimensional-modeling/.claude-plugin/plugin.json
2. Created SKILL.md (166 lines, 997 words -- well under 500/5000 limits)
   - 8-step process: identify business process, declare grain, identify dimensions, design facts, generate keys, implement SCD Type 2, build views, add meta tables
   - Agent execution as DAG section mapping five invariant operations to fact tables
   - References to 5 reference docs
3. Created 5 reference files:
   - schema_patterns.md: dimension, fact, bridge, meta table templates with working examples from store.py
   - query_patterns.md: 12 query patterns (basic joins, latest-per-entity, time-bounded, trend analysis, SCD point-in-time, multi-fact, freshness, FILTER clause, recursive CTE, budget trend, load logging)
   - key_generation.md: dimension_key(), hash_diff(), natural vs surrogate, composite keys, NULL handling, degenerate dimensions
   - anti_patterns.md: 10 anti-patterns (PK on SCD2, sequences, FK constraints, normalizing facts, missing metadata, timestamp keys, single event table, wide dimensions, no hash_diff, treating DuckDB as row store)
   - dag_execution.md: full DAG schema (6 fact tables), capture mechanism (hooks + journal.py), 3 analytical views
4. Created README.md with installation, skills table, invocation examples
5. Validated with skills-ref: PASS

### phase 2b: metadata updates
1. Added dimensional-modeling to .claude-plugin/marketplace.json
2. Added v0.7.0 entry to CHANGELOG.md
3. Bumped pyproject.toml version to 0.7.0
4. Added to root README.md: skills table, installation commands, usage section

### verification
- [x] star-schema-llm-context README answers "what/who/why" in 3 paragraphs
- [x] Dead code deleted (~3950 lines)
- [x] dimensional-modeling SKILL.md validates with skills-ref
- [x] SKILL.md under 500 lines / 5000 words (166 / 997)
- [x] References cover core patterns (schema, query, keys, anti-patterns, DAG execution)
- [x] Plugin added to marketplace.json and root README
- [x] Session logged

### files created (star-schema-llm-context)
- `pyproject.toml`

### files modified (star-schema-llm-context)
- `README.md` (rewrite)
- `CLAUDE.md` (rewrite)
- `docs/library_design.md` (expansion roadmap replaced)

### files deleted (star-schema-llm-context)
- `graph_algorithms.py`, `mcp_server.py`, `schema.sql`, `db_manager.py`
- `setup.py`, `requirements.txt`, `Makefile`, `ARCHITECTURE.md`, `config.yaml`

### files created (fb-claude-skills)
- `dimensional-modeling/.claude-plugin/plugin.json`
- `dimensional-modeling/skills/dimensional-modeling/SKILL.md`
- `dimensional-modeling/skills/dimensional-modeling/references/schema_patterns.md`
- `dimensional-modeling/skills/dimensional-modeling/references/query_patterns.md`
- `dimensional-modeling/skills/dimensional-modeling/references/key_generation.md`
- `dimensional-modeling/skills/dimensional-modeling/references/anti_patterns.md`
- `dimensional-modeling/skills/dimensional-modeling/references/dag_execution.md`
- `dimensional-modeling/README.md`

### files modified (fb-claude-skills)
- `.claude-plugin/marketplace.json`
- `CHANGELOG.md`
- `pyproject.toml`
- `README.md`
- `internal/log/log_2026-02-14.md`

### what I'd do differently
Nothing significant. The plan was clear and well-scoped. The critique synthesis informed the right cuts (embeddings, graph DB) and the right additions (DAG execution model, automation patterns). The skill is deliberately a "teaching skill" -- it doesn't run code, it guides schema design. This is the right abstraction for now: build demand through usage before extracting a library.

### future sessions
- **Phase 3**: Test the skill with Agent Task Decomposition use case (invoke /dimensional-modeling, follow guidance to design schema, implement, set up hook-based capture)
- **Phase 4**: Update star-schema-llm-context library_design.md with DAG execution model as reference consumer
- **Library extraction**: after 2+ working consumers use the patterns, extract into src/star_schema/ package

## doc updates (continuation session)

### context
Reviewed external analysis of the star-schema-llm-context vision. Folded useful bits into existing docs naturally without attribution or hype.

### what was done
1. **library_design.md**: added write-back paragraph to DAG execution model section -- dimension mutability means you can fix routing by changing data, not code
2. **abstraction_analogies.md**: fixed stale implementation status (was "prototype with flat schema, requirements.txt" -- now reflects cleaned-up state with pyproject.toml and deleted dead code). Updated fb-claude-skills version to v0.7.0 and added dimensional-modeling skill to the list.

### files modified (star-schema-llm-context)
- `docs/library_design.md`
- `docs/abstraction_analogies.md`
